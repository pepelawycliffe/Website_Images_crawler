{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter URL with images : https://www.spu.ac.ke/new/\n",
      "Extracting from www.spu.ac.ke ...\n",
      "\n",
      "Should we can scan entire site or just home page ?\n",
      "1. Entire site\n",
      "2.Just this page\n",
      "Option : 1\n",
      "https://www.matthew28ministries.org\n",
      "Grab occured while getting links\n",
      "HTTPSConnectionPool(host='www.matthew28ministries.org', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000217B28A87C0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed'))\n",
      "https://www.wvi.org/kenya\n",
      "Done\n",
      "http://www.spu.ac.ke/blog/\n",
      "Done\n",
      "https://www.ackenya.org/\n",
      "Done\n",
      "https://www.google.com/a/spu.ac.ke/ServiceLogin?service=mail&passive=true&rm=false&continue=https://mail.google.com/a/spu.ac.ke/&ss=1<mpl=default<mplcache=2\n",
      "Done\n",
      "https://spu.co.ke\n",
      "Done\n",
      "https://students.spu.ac.ke/\n",
      "Done\n",
      "https://www.procmura.org\n",
      "Done\n",
      "https://kiambu.go.ke/\n",
      "Done\n",
      "https://medicalmission.org/\n",
      "Done\n",
      "http://www.spu.ac.ke/new/nakuru\n",
      "Done\n",
      "http://www.spu.ac.ke/old/academic-registry/verify-certificates.html\n",
      "Done\n",
      "https://staff.spu.ac.ke/\n",
      "Done\n",
      "https://vlearning.spu.ac.ke/\n",
      "Grab occured while getting links\n",
      "HTTPSConnectionPool(host='vlearning.spu.ac.ke', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)')))\n",
      "http://journals.spu.ac.ke/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ImageSpider:\n",
    "    def __init__(self):\n",
    "        self.home = os.getcwd()\n",
    "    \n",
    "    def grab_all_image_links(self, URL):\n",
    "        try:\n",
    "            valid_links = []\n",
    "            url_protocol = URL.split('/')[0]\n",
    "            url_html = requests.get(URL).text\n",
    "            Image_urls = re.findall(r'((http\\:|https\\:)?\\/\\/[^\"\\' ]*?\\.(png|jpg))', url_html, flags=re.IGNORECASE | re.MULTILINE | re.UNICODE)\n",
    "            for image in Image_urls:\n",
    "                image_url = image[0]\n",
    "                if not image_url.startswith(url_protocol):\n",
    "                    image_url = url_protocol+image_url\n",
    "                    valid_links.append(image_url)\n",
    "                else:\n",
    "                    valid_links.append(image_url)\n",
    "            print('Done')\n",
    "        except Exception as graberror:\n",
    "            print('Grab occured while getting links')\n",
    "            print(graberror)\n",
    "            return []  \n",
    "        return valid_links\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_image_name(url):\n",
    "        image_name = str(url).split('/')[-1]\n",
    "        return image_name\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_site_name(url):\n",
    "        sitename = str(url).split('/')[2]\n",
    "        return sitename\n",
    "    \n",
    "    def saving_images(self,url):\n",
    "        Image_links = self.grab_all_image_links(url)\n",
    "        for link in Image_links:\n",
    "            raw_image = requests.get(link, stream=True).raw\n",
    "            img = Image.open(raw_image)\n",
    "            image_name = self.extract_image_name(link)\n",
    "            img.save(image_name)\n",
    "    \n",
    "    def grab_all_links(self, url):\n",
    "        links = [url]\n",
    "        link_html = requests.get(url).text\n",
    "        all_links = BeautifulSoup(link_html, 'html.parser').findAll('a')\n",
    "        for link in all_links:\n",
    "            href = link.get('href')\n",
    "            if href:       \n",
    "                if href.startswith('http') or href.startswith('https'):\n",
    "                    links.append(href)\n",
    "        return links\n",
    "\n",
    "    def download_images(self):\n",
    "        url = input('Enter URL with images : ')\n",
    "        try:\n",
    "            sitename = self.extract_site_name(url)\n",
    "            print('Extracting from {} ...'.format(sitename))\n",
    "            os.mkdir(sitename);os.chdir(sitename)\n",
    "            print('\\nShould we can scan entire site or just home page ?')\n",
    "            option = int(input('1. Entire site\\n2.Just this page\\nOption : '))\n",
    "            if option == 1:\n",
    "                all_avaialble_links = set(self.grab_all_links(url))\n",
    "            else:\n",
    "                all_avaialble_links = [url]\n",
    "            for link in all_avaialble_links:\n",
    "                try:                        \n",
    "                    print(link)\n",
    "                    self.saving_images(link)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        except Exception as Error:\n",
    "            print('Error occured while grabing site links')\n",
    "            print(Error)\n",
    "\n",
    "        finally:\n",
    "            print('Scraping finished')\n",
    "            os.chdir(self.home)\n",
    "\n",
    "\n",
    "spider = ImageSpider()\n",
    "spider.download_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
